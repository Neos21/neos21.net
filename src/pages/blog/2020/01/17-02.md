---
title        : ログファイルに同一行が何行あるかカウントするワンライナー
created      : 2020-01-17
last-modified: 2020-01-17
header-date  : true
path:
  - /index.html Neo's World
  - /blog/index.html Blog
  - /blog/2020/index.html 2020年
  - /blog/2020/01/index.html 01月
hidden-info:
  original-blog: Corredor
---

アクセスログのファイルなんかを見ていて、「12時台に一番アクセスがあったページはどれかな？」みたいなのを探る時に、コマンドで探る方法を紹介する。

- `example.log`

```
2020-01-01T00:01:24 /index.html    200
2020-01-01T00:01:25 /somepage.html 200
2020-01-01T00:01:31 /index.html    200
2020-01-01T00:01:35 /mail.html     200
2020-01-01T00:01:36 /index.html    200
2020-01-01T00:01:37 /mail.html     200
2020-01-01T00:01:38 /index.html    200
2020-01-01T00:01:40 /index.html    200

……こういう感じの形式のログ
```

このログだと、2列目に「アクセスがあったページのパス」が出力されている。そこで、次のようなコマンドで、2列目のパス部分だけを抽出する。

```bash
$ cat example.log | cut -d ' ' -f2

/index.html
/somepage.html
/index.html
/mail.html
/index.html
/mail.html
/index.html
/index.html
```

コレをベースに使うのは、次のコマンド。

```bash
sort | uniq -c | sort -n
```

この **`sort | uniq -c | sort -n`** というイディオムで、「*重複する行の数をカウント*」できる。実際にやってみよう。

```bash
$ cat example.log | cut -d ' ' -f2 | sort | uniq -c | sort -n

   1 /somepage.html
   2 /mail.html
   5 /index.html
```

このとおり。これでいくと、`/index.html` というパスへのアクセス数が一番多い、ということが簡単に分かる。

`uniq -c` で重複する行数を出力してくれる。「何で事前に `sort` するの？」というと、*`uniq` コマンドは予め `sort` したデータでないと正常に処理してくれない*ためだ。

`sort -n` の `-n` は何かというと、数値での並べ替えを正しい数値順に行うためのモノ。コレを指定しないと、`1` → `10` → `19` → `2` → `20` …という風に並んでしまう。

-----

とりあえずコレで簡単に「重複行の集計」が出来た。

- 参考 : [【 uniq 】コマンド――重複している行を削除する : Linux基本コマンドTips（64） - ＠IT](https://www.atmarkit.co.jp/ait/articles/1611/14/news021.html)
- 参考 : [awkガナス - 同じ行がいくつあるかカウントする(awk+sort+uniq) | 株式会社創夢 — SOUM/misc](https://www.soum.co.jp/misc/awk/3/)
- 参考 : [sort, sort -n, sort -r, sort -nr の違い](http://www.nemotos.net/?p=2829)
